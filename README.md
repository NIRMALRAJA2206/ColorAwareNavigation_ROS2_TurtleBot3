# Color-Aware Maze Navigation using ROS 2 and TurtleBot3
**Author:** Nirmal Raja Karuppiah Loganathan

---

## üöÄ Project Overview

This project demonstrates a complete **ROS 2-based autonomous navigation** system for a TurtleBot3 robot. The robot follows predefined waypoints while performing real-time **obstacle avoidance based on object color** (red or blue).

### Two Main Runs:
- **Basic Run** (`maze_solver.py`) ‚Äì Straightforward waypoint following.
- **Optimized Run** (`msg.py`) ‚Äì Performs lateral detours using color-aware logic.

The robot:
- Uses **SLAM Toolbox** to build the map.
- Navigates in a **custom Gazebo maze world**.
- Uses **LiDAR and RGB camera fusion** to detect red/blue obstacles.
- Performs side-steps: left for blue, right for red.

---

## üß± Maze World Setup

A custom maze was designed in Gazebo with:
- Walls to form narrow paths.
- Red and blue cylinders placed to block default paths.

This forced a **visual contrast** between the basic and color-aware runs:
- The first run ignores object color.
- The second run interprets object color to decide detour direction.

---

## üó∫Ô∏è Mapping & First Run (`maze_solver.py`)

1. **SLAM Map Generation**:
   - TurtleBot3 follows a vertical list of waypoints to build the map using `slam_toolbox`.
   - SLAM is run in async mode.

2. **Basic Waypoint Navigation**:
   - After the map is saved, the robot is relaunched.
   - It follows the same waypoints again with no avoidance logic.

This run acts as a baseline to compare the performance of the color-aware version.

---

## üéØ Color-Aware Navigation (`msg.py`)

In this run, the robot performs **dynamic object avoidance** using:
- Real-time RGB camera data
- HSV color masking
- LiDAR fusion
- FSM-based detour logic

---

### üîç Detection Pipeline

- **Color Masking (OpenCV + HSV)** for:
  - Red: Ranges [0‚Äì10] and [160‚Äì179]
  - Blue: Range [100‚Äì140]
- **Blob Centroid Calculation**
- **Angle Estimation** based on camera HFOV
- **Distance Estimation** from LiDAR
- **Debounce Logic** for stable detection:
  - Contour area threshold
  - Object distance < 2 feet
  - Multiple consistent frames

---

### üìê Lateral Shift Calculation

The offset for side-step is computed using:

```math
FOV_rad = (œÄ / 180) √ó FOV

PPM = W / (2 √ó tan(FOV_rad / 2) √ó D)

w_px = sqrt(A)

offset_px = |(w_px / 2) - (W / 2)|

LateralShift_m = offset_px / PPM

```
---

### üìê Variable Definitions

**Where:**

- `W` = Image width (px)  
- `FOV` = Camera field of view (¬∞)  
- `D` = Object distance (m) from LiDAR  
- `A` = Contour area (px¬≤)  
- `w_px` = Estimated object width (px)

---

### ‚öôÔ∏è FSM-Based Behavior Logic

| Color Detected | Action          |
|----------------|-----------------|
| üîµ Blue        | Side-step Left  |
| üî¥ Red         | Side-step Right |

The robot computes the new goal by **translating its current position laterally** and resumes the navigation to the next waypoint.

---

### üìÇ Resources

- **GitHub Repo**: [Source Code](https://github.com/your-username/turtlebot3-color-aware-navigation)
- **Demo Videos**:
  - [üó∫Ô∏è SLAM Mapping + First Run](https://drive.google.com/file/d/1UaMZxqPJT6QfQmIylV5r0fx4cvESgiKK/view?usp=sharing)
  - [üéØ Optimized Color-Aware Navigation](https://drive.google.com/file/d/1tP28uWU_Dt9cDNH5ABu3sfvdU57wGCHI/view?usp=sharing)

---

### ‚úÖ Conclusion

This project shows that a TurtleBot3 can:

- ‚úÖ Follow waypoints with Nav2  
- ‚úÖ Build a 2D map using SLAM Toolbox  
- ‚úÖ Detect red and blue objects using real-time RGB + LiDAR data  
- ‚úÖ Perform lateral detours based on perceived color and location  

The visual and logical contrast between the two runs clearly demonstrates the value of the color-aware logic.

**Future improvements may include:**

- üîß More robust detection in low light  
- üîÑ Smoother detour transitions  
- üß† Adaptive thresholds based on environment

---
This README was formatted with assistance from [ChatGPT](https://chat.openai.com/)

